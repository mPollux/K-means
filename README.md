# ğŸ“˜ **K-means 1D â€” VersÃµes Sequencial, OpenMP, CUDA e MPI**

Este projeto implementa o algoritmo **K-means 1D** em mÃºltiplas arquiteturas de paralelizaÃ§Ã£o:

* **Naive (Sequencial â€“ baseline)**
* **OpenMP (CPU multithread)**
* **CUDA (GPU)**
* **MPI (processamento distribuÃ­do com mÃºltiplos processos)**

O objetivo Ã© avaliar:

* **Strong scaling**: desempenho Ã  medida que aumentamos P (processos/threads)
* **Custo de comunicaÃ§Ã£o** (ex.: Allreduce no MPI)
* **Throughput (pontos/s)**
* **Speedup**
* **Corretude (comparaÃ§Ã£o de SSE)**

Toda a execuÃ§Ã£o, coleta de dados e anÃ¡lise grÃ¡fica Ã© automatizada pelos scripts incluÃ­dos.

---

# ğŸ“ **Estrutura do RepositÃ³rio**

```
.
â”œâ”€â”€ conjuntos_teste/
â”‚   â”œâ”€â”€ pipeline.sh
â”‚   â”œâ”€â”€ dados_p.csv
â”‚   â”œâ”€â”€ dados_m.csv
â”‚   â”œâ”€â”€ dados_g.csv
â”‚   â”œâ”€â”€ centroides_iniciais_p.csv
â”‚   â”œâ”€â”€ centroides_iniciais_m.csv
â”‚   â””â”€â”€ centroides_iniciais_g.csv
â”‚
â”œâ”€â”€ serial/
â”‚   â””â”€â”€ kmeans_1d_naive.c
â”‚
â”œâ”€â”€ openmp/
â”‚   â””â”€â”€ kmeans_1d_omp.c
â”‚
â”œâ”€â”€ cuda/
â”‚   â””â”€â”€ kmeans_1d_cuda.cu
â”‚
â”œâ”€â”€ mpi/
â”‚   â””â”€â”€ kmeans_1d_mpi.c
â”‚
â”œâ”€â”€ run_bench.sh               # Script unificado de benchmark
â”œâ”€â”€ analisar_bench.py          # GrÃ¡ficos, tabelas, speedups e validaÃ§Ã£o
â”œâ”€â”€ figs_bench/                # SaÃ­da automÃ¡tica dos grÃ¡ficos
â”‚   â”œâ”€â”€ openmp/
â”‚   â”œâ”€â”€ cuda/
â”‚   â”œâ”€â”€ mpi/
â”‚   â””â”€â”€ global/
â””â”€â”€ README.md
```

---

# ğŸ“Œ **DescriÃ§Ã£o das VersÃµes e MÃ©tricas**

## ğŸ”¹ **1. Naive (Sequencial â€“ CPU)**

VersÃ£o bÃ¡sica usada como baseline.

**MÃ©tricas:**

* Tempo total
* IteraÃ§Ãµes
* SSE final (para verificaÃ§Ã£o de corretude)

---

## ğŸ”¹ **2. OpenMP (CPU multithread)**

ConfiguraÃ§Ãµes testadas:

* Threads: `1, 2, 4, 8, 16`
* Schedules: `static`, `dynamic`
* Chunk sizes: `1, 64, 256, 1024`

**MÃ©tricas:**

* Tempo (mediana de 5 execuÃ§Ãµes)
* Speedup vs. sequencial
* Throughput (pontos/s)
* SSE final
* ComparaÃ§Ã£o de escalonamento e chunk

---

## ğŸ”¹ **3. CUDA (GPU)**

ImplementaÃ§Ã£o paralela com kernels CUDA.

**MÃ©tricas:**

* H2D, Kernel, D2H
* Tempo total
* Throughput
* Speedup vs. sequencial e vs OpenMP
* Grid size e block size

---

## ğŸ”¹ **4. MPI (Processos distribuÃ­dos)**

VersÃ£o paralela com **MPI**, baseada na divisÃ£o do vetor de pontos entre os processos.

Cada iteraÃ§Ã£o faz:

1. **Broadcast** dos centrÃ³ides (C)
2. **Assignment local** em cada processo
3. **ReduÃ§Ãµes globais**:

   * `MPI_Reduce` para SSE
   * `MPI_Allreduce` para somas e contagens
4. **Update global** dos centrÃ³ides

**MÃ©tricas extraÃ­das:**

* Tempo total
* Tempo de comunicaÃ§Ã£o (Allreduce)
* Tempo de computaÃ§Ã£o aproximado
* Strong scaling para P = 1, 2, 4, 8, â€¦
* Speedup vs. sequencial
* Speedup vs. melhor OpenMP
* Throughput (pontos/s)

---

# ğŸš€ Como Executar

## 1ï¸âƒ£ **Gerar conjuntos de teste**

```bash
cd conjuntos_teste
chmod +x pipeline.sh
./pipeline.sh
```

---

# 2ï¸âƒ£ **Executar os benchmarks**

O script unificado aceita flags:

* `--omp`
* `--cuda`
* `--mpi`
* `--all`

### ğŸ”¸ Somente Sequencial + MPI

```bash
./run_bench.sh --mpi
```

### ğŸ”¸ Sequencial + OpenMP

```bash
./run_bench.sh --omp
```

### ğŸ”¸ Sequencial + CUDA

```bash
./run_bench.sh --cuda
```

### ğŸ”¸ Todas as versÃµes (seq + omp + cuda + mpi)

```bash
./run_bench.sh --all
```

### ğŸ“Œ SaÃ­da do script

Gera arquivos no formato:

```
resultados_omp_mpi_YYYYMMDD_HHMMSS.csv
resultados_omp_cuda_mpi_YYYYMMDD_HHMMSS.csv
resultados_mpi_YYYYMMDD_HHMMSS.csv
```

Incluindo:

* tempos
* iteraÃ§Ãµes
* SSE final
* tempo de comunicaÃ§Ã£o (MPI)
* throughput
* parÃ¢metros (threads, blocks, processos, schedule)

---

# 3ï¸âƒ£ **Gerar grÃ¡ficos e tabelas**

Novo formato:

```
python3 analisar_bench.py <arquivo_csv> --mpi
python3 analisar_bench.py <arquivo_csv> --openmp
python3 analisar_bench.py <arquivo_csv> --cuda
python3 analisar_bench.py <arquivo_csv> --all
```

### ğŸ”¸ Exemplo: comparar **naive Ã— MPI**

```bash
python3 analisar_bench.py resultados_mpi_YYYYMMDD_HHMMSS.csv --mpi
```

### ğŸ”¸ Rodar tudo

```bash
python3 analisar_bench.py resultados_omp_cuda_mpi_YYYYMMDD_HHMMSS.csv --all
```

---

# ğŸ“Š Estrutura de SaÃ­da dos GrÃ¡ficos

```
figs_bench/
â”œâ”€â”€ openmp/
â”‚   â”œâ”€â”€ p_omp_*.png
â”‚   â”œâ”€â”€ m_omp_*.png
â”‚   â””â”€â”€ g_omp_*.png
â”œâ”€â”€ cuda/
â”‚   â”œâ”€â”€ p_cuda_*.png
â”‚   â”œâ”€â”€ m_cuda_*.png
â”‚   â””â”€â”€ g_cuda_*.png
â”œâ”€â”€ mpi/
â”‚   â”œâ”€â”€ p_mpi_tempo_vs_procs.png
â”‚   â”œâ”€â”€ p_mpi_speedup_vs_procs.png
â”‚   â””â”€â”€ p_mpi_breakdown_vs_procs.png
â””â”€â”€ global/
    â””â”€â”€ comparacao_seq_omp_cuda_mpi.csv
```

### **MPI â€“ grÃ¡ficos incluÃ­dos**

* **Tempo total vs processos** (Strong scaling)
* **Speedup vs sequencial**
* **Tempo total Ã— comunicaÃ§Ã£o (Allreduce) Ã— computaÃ§Ã£o**

Esses grÃ¡ficos atendem exatamente aos requisitos do enunciado:

âœ” Strong scaling
âœ” Tempo de comunicaÃ§Ã£o
âœ” Speedup vs serial e vs OpenMP

---

# ğŸ§° DependÃªncias

### CompilaÃ§Ã£o:

* GCC com OpenMP
* NVCC (para CUDA)
* MPI (OpenMPI ou MPICH)

CompilaÃ§Ã£o manual:

```bash
mpicc -O2 -std=c99 kmeans_1d_mpi.c -o kmeans_1d_mpi -lm
```

### Ambiente Python:

```
pip install -r requirements.txt
```